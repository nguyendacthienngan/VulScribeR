{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !conda create --name vulscriber-cluster-env python=3.8\n",
    "# !conda activate vulscriber-cluster-env\n",
    "# !conda install pytorch==1.10.0 torchvision==0.11.1 torchaudio==0.10.0 cudatoolkit=11.3 -c pytorch\n",
    "# !conda install transformers==4.37.2 pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.cuda.is_available())  # Should print: True\n",
    "# print(torch.cuda.device_count())  # Should print: 1\n",
    "# print(torch.cuda.get_device_name(0))  # Should print: NVIDIA GeForce RTX 3060\n",
    "# print(torch.cuda.current_device())  # Should print 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\vulscriber-cluster-env\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:35<01:47, 35.80s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "from chat import PromptUtils\n",
    "\n",
    "extract_code_content = PromptUtils.extract_code_content\n",
    "\n",
    "from chat import CodeQwenPrompter as code_qwen\n",
    "from chat import PromptUtils as pu\n",
    "\n",
    "ch = code_qwen(lock=None) # Selects CodeQwen1.5--7B-Chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the LLM and the bigvul's train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 1813)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load file JSON và chuyển thành DataFrame\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(MEGAVUL_CLEAN_TRAIN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 10\u001b[0m     clean_json_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m dftr \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(clean_json_data)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\vulscriber-cluster-env\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\vulscriber-cluster-env\\lib\\json\\__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\vulscriber-cluster-env\\lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 1813)"
     ]
    }
   ],
   "source": [
    "# Load file JSON và chuyển thành DataFrame\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "MEGAVUL_CLEAN_TRAIN = r\"C:\\Users\\Administrator\\Downloads\\VulScribeR-main\\code\\dataset\\megavul\\new\\megavul_clean.json\"\n",
    "\n",
    "\n",
    "# Load file JSON và chuyển thành DataFrame\n",
    "with open(MEGAVUL_CLEAN_TRAIN, \"r\", encoding=\"utf-8\") as f:\n",
    "    clean_json_data = json.load(f)\n",
    "dftr = pd.DataFrame(clean_json_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduciton of RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from rag import read_naive_full_header_rag, read_naive_full_bigvul_header_rag, read_naive_full_func_rag, read_naive_code2code_rag, read_naive_code2code_ext_rag, read_random, read_naive_code2code_clustered_rag, read_random_fair\n",
    "\n",
    "search_results = read_naive_code2code_clustered_rag() # For Injection and Extension Methods - RQ1 and RQ3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7059,\n",
       " SearchItem(searchItemId='src/C/cholmod.c', scoredCodeSnippets=[\n",
       "   ScoredCodeSnippet(id='mono/metadata/icall.c', body='static gboolean\n",
       " ves_icall_System_Array_FastCopy (MonoArray *...', header='static gboolean ves_icall_System_Array_FastCopy (MonoArray* source, int source_idx, MonoArray* dest, int dest_idx, int length)', cluster='0', score=4.092301)\n",
       "   ScoredCodeSnippet(id='codestream/acsequentialscan.hpp', body='void Init(bool hi) \n",
       "       {\n",
       "         for(int i = 0;i < 18;i++...', header='void Init(bool hi)', cluster='1', score=3.7259467)\n",
       "   ScoredCodeSnippet(id='src/C/cholmod.c', body='static PyObject* spsolve(PyObject *self, PyObject *args,\n",
       "    ...', header='static PyObject* spsolve(PyObject* self, PyObject* args, PyObject* kwrd)', cluster='2', score=18.262472)\n",
       "   ScoredCodeSnippet(id='src/imagew-bmp.c', body='static int find_high_bit(unsigned int x)\n",
       " {\n",
       " \tint i;\n",
       " \tfor(i=31...', header='static int find_high_bit(unsigned int x)', cluster='3', score=4.4920135)\n",
       "   ScoredCodeSnippet(id='libavcodec/vorbis.c', body='static void render_line(int x0, int y0, int x1, int y1, floa...', header='static void render_line(int x0, int y0, int x1, int y1, float* buf)', cluster='4', score=5.065495)\n",
       " ]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0].max_score, search_results[1].max_score, search_results[2].max_score, search_results[10].max_score, search_results[100].max_score \n",
    "len(search_results),search_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì MegaVul không có trường metadata riêng cho vulnerable_lines, chúng ta sẽ giả định rằng các dòng code bị xóa (deleted_lines) trong quá trình sửa lỗi (tức là trong diff_line_info) chính là các dòng chứa hoặc liên quan trực tiếp đến lỗ hổng trong phiên bản code trước đó (func_before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vulnerable_lines_from_diff(diff_info):\n",
    "    \"\"\"Extracts deleted lines from diff_info as vulnerable lines.\"\"\"\n",
    "    if not diff_info or \"deleted_lines\" not in diff_info:\n",
    "        return []\n",
    "    return diff_info[\"deleted_lines\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7059\n",
      "35167\n"
     ]
    }
   ],
   "source": [
    "from rag import flatten_search_items\n",
    "from operator import attrgetter\n",
    "\n",
    "def sort(search_results):\n",
    "    return sorted(search_results, key=attrgetter('max_score'), reverse=True)\n",
    "\n",
    "print(len(search_results)) \n",
    "search_results = flatten_search_items(search_results) # flattens search results in case the format is (1 clean) -> 5 vuls (for RQ2) instead of 1-1 matching, otherwise it doesn't change anything\n",
    "\n",
    "# Extracts deleted lines from diff_info, assuming they are the vulnerable lines.\n",
    "# for result in search_results:\n",
    "#     print(result)\n",
    "#     # diff_info = result.get(\"diff_line_info\", {})\n",
    "#     diff_info = getattr(result, \"diff_line_info\", {})\n",
    "#     print(diff_info)\n",
    "#     result[\"vulnerable_lines\"] = get_vulnerable_lines_from_diff(diff_info)  # Store extracted lines\n",
    "\n",
    "search_results = sort(search_results) # sort the results (if clustering is used, this will be invalidated and the clustered sampling algorithm's sorting method will be used)\n",
    "print(len(search_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.350285964307089 10.350285964307089 89.021286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg = 0\n",
    "max_avg = 0\n",
    "avg_m = 0\n",
    "for r in search_results:\n",
    "    if avg_m < r.avg_score:\n",
    "        avg_m = r.avg_score\n",
    "    avg += r.avg_score / len(search_results)\n",
    "    max_avg += r.max_score/len(search_results)\n",
    "\n",
    "print(avg, max_avg, avg_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusted Sampling was used...\n"
     ]
    }
   ],
   "source": [
    "def clustered_sampling(search_results):\n",
    "    if search_results[0].scoredCodeSnippets[0].cluster == -1:\n",
    "        print(\"Search results are for a non-clustered strategy, skipping clustred_sampling\")\n",
    "        return search_results\n",
    "\n",
    "    cluster_order = [4,3,1,0,2] # sorted based on size\n",
    "    cluster_indices = [0,0,0,0,0]\n",
    "\n",
    "    new_search_results = []\n",
    "\n",
    "    search_results_per_cluster = {0:[], 1:[], 2:[],3:[],4:[]}\n",
    "\n",
    "    for sr in search_results:\n",
    "        search_results_per_cluster[sr.scoredCodeSnippets[0].cluster].append(sr) # putting items of each cluster to it's array\n",
    "\n",
    "    for i in range(5):\n",
    "        search_results_per_cluster[i] = sort(search_results_per_cluster[i]) # sort each cluster's results\n",
    "\n",
    "    for i in range(len(search_results)):\n",
    "        cluster_number = cluster_order[i%5] # sampling from sorted clusters while starting from the largest cluster\n",
    "        cluster_index = cluster_indices[cluster_number]\n",
    "\n",
    "        if cluster_index < len(search_results_per_cluster[cluster_number]):\n",
    "            new_search_results.append(search_results_per_cluster[cluster_number][cluster_index])\n",
    "            cluster_indices[cluster_number] += 1\n",
    "    \n",
    "    print(\"Clusted Sampling was used...\")\n",
    "    return new_search_results\n",
    "\n",
    "search_results = clustered_sampling(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.94688, 41.326096, 66.86675, 74.269684, 56.64057)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0].max_score, search_results[1].max_score, search_results[2].max_score, search_results[10].max_score, search_results[100].max_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resuming_data_frame = pd.read_json(\"./generated/\"somesetting\".jsonl\", orient=\"records\", lines=True) # just in case more was needed, but didn't want to start from 0 again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 109\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pu\u001b[38;5;241m.\u001b[39mget_mutation_prompt(vul_code, flaw_lines)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Generator Phase (can send concurrent requests)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m indices, vuls, cleans, vul_lines, generated \u001b[38;5;241m=\u001b[39m cu\u001b[38;5;241m.\u001b[39mconcurrent_prompter(\n\u001b[0;32m    104\u001b[0m     prompt_creator_from_searchItem\u001b[38;5;241m=\u001b[39mget_injection_strategy_devign,\n\u001b[0;32m    105\u001b[0m     index_retreiver\u001b[38;5;241m=\u001b[39mget_index,\n\u001b[0;32m    106\u001b[0m     vul_retriever\u001b[38;5;241m=\u001b[39mget_vul,\n\u001b[0;32m    107\u001b[0m     clean_retriever\u001b[38;5;241m=\u001b[39mget_clean_megavul,\n\u001b[0;32m    108\u001b[0m     vul_lines_retriever\u001b[38;5;241m=\u001b[39mget_vul_lines,\n\u001b[1;32m--> 109\u001b[0m     search_results\u001b[38;5;241m=\u001b[39m\u001b[43msearch_results\u001b[49m,\n\u001b[0;32m    110\u001b[0m     llm\u001b[38;5;241m=\u001b[39mch, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6000\u001b[39m, max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, is_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, resume_from_dataframe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# for code Qwen set workers to 1 as there's only one model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'search_results' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rag import SearchItem\n",
    "from chat import ConcurrencyUtils as cu\n",
    "\n",
    "def get_index(search_item: SearchItem):\n",
    "    return str(search_item.best)\n",
    "\n",
    "\n",
    "def get_vul(search_item: SearchItem):\n",
    "    best_target_index = search_item.best\n",
    "    vul_record = dftr[dftr['file_path'] == best_target_index]\n",
    "    return str(vul_record['func_before'].values[0])\n",
    "\n",
    "def get_fixed_vul(search_item: SearchItem):\n",
    "    best_target_index = search_item.best\n",
    "    vul_record = dftr[dftr['file_path'] == best_target_index]\n",
    "    return str(vul_record['func'].values[0])\n",
    "\n",
    "# def get_vul_lines(search_item: SearchItem):\n",
    "#     best_target_index = search_item.best\n",
    "#     vul_record = dftr[dftr['file_path'] == best_target_index]\n",
    "#     flaw_lines = str(vul_record['diff_line_info'].values[0]).strip()\n",
    "#     if len(flaw_lines) < 5:\n",
    "#         raise Exception(f\"Invalid Vul_lines: {flaw_lines}\")\n",
    "#     return flaw_lines\n",
    "def get_vul_lines(search_item: SearchItem):\n",
    "    best_target_index = search_item.best\n",
    "    vul_record = dftr[dftr['file_path'] == best_target_index]\n",
    "\n",
    "    # Debugging step: Print matched rows\n",
    "    print(f\"Matching records for {best_target_index}:\")\n",
    "    print(vul_record)\n",
    "\n",
    "    # Check if record exists\n",
    "    if vul_record.empty:\n",
    "        raise Exception(f\"No matching record found for file_path: {best_target_index}\")\n",
    "\n",
    "    # Check if `diff_line_info` exists\n",
    "    if \"diff_line_info\" not in vul_record.columns:\n",
    "        raise Exception(f\"Missing `diff_line_info` column in dftr!\")\n",
    "\n",
    "    # Extract flaw lines\n",
    "    flaw_lines = vul_record['diff_line_info'].values[0]\n",
    "\n",
    "    # Handle missing or empty flaw_lines\n",
    "    if flaw_lines is None or str(flaw_lines).strip() == \"\":\n",
    "        raise Exception(f\"Missing or empty `diff_line_info` for: {best_target_index}\")\n",
    "\n",
    "    return str(flaw_lines).strip()\n",
    "\n",
    "\n",
    "\n",
    "# def get_clean_bigvul(search_item: SearchItem):\n",
    "#     best_target_index = int(search_item.best)\n",
    "#     return str(dftr[dftr['index'] == best_target_index]['processed_func'].values[0])\n",
    "\n",
    "# def get_clean_ext(search_item: SearchItem):\n",
    "#     retrieved = None\n",
    "#     try:\n",
    "#         retrieved = get_clean_megavul(search_item)\n",
    "#     except Exception as e:\n",
    "#         # print(f\">>getting clean item, devign wasn't found {e}\")\n",
    "#         retrieved = get_clean_bigvul(search_item)\n",
    "#         # print(\"<<retrieved from bigvul\")\n",
    "#         assert retrieved != None and len(retrieved) > 0\n",
    "#     finally:\n",
    "#         return retrieved\n",
    "\n",
    "# def get_clean_megavul(search_item: SearchItem):\n",
    "#     with open(f\"./../devign_code/{search_item.searchItemId}\", 'r') as file:\n",
    "#         clean_item = file.read()\n",
    "#     return clean_item\n",
    "\n",
    "\n",
    "def get_clean_megavul(search_item: SearchItem):\n",
    "    # Search for the function by matching `searchItemId` with the `file_path` key\n",
    "    for item in clean_json_data:\n",
    "        if item.get(\"file_path\") == search_item.searchItemId:\n",
    "            return item.get(\"func\", None)  # Return function content if found\n",
    "\n",
    "    return None  # Return None if not found\n",
    "\n",
    "# Formulator Functions that take a search result and populates a prompt template\n",
    "\n",
    "def get_injection_strategy_devign(search_item: SearchItem): # the Injection strategy while the clean's come from devign\n",
    "    clean_item = get_clean_megavul(search_item)\n",
    "    vul_code = get_vul(search_item)\n",
    "    flaw_lines = get_vul_lines(search_item)\n",
    "    return pu.get_full_prompt(vul_code, clean_item, flaw_lines)\n",
    "\n",
    "\n",
    "def get_extension_strategy_devign(search_item: SearchItem):\n",
    "    clean_item = get_clean_megavul(search_item)\n",
    "    vul_code = get_vul(search_item)\n",
    "    flaw_lines = get_vul_lines(search_item)\n",
    "    return pu.get_ext_wl_prompt(clean_item, vul_code, flaw_lines)\n",
    "\n",
    "def get_mutation_strategy(search_item: SearchItem):\n",
    "    vul_code = get_vul(search_item)\n",
    "    flaw_lines = get_vul_lines(search_item)\n",
    "    return pu.get_mutation_prompt(vul_code, flaw_lines)\n",
    "\n",
    "\n",
    "# Generator Phase (can send concurrent requests)\n",
    "indices, vuls, cleans, vul_lines, generated = cu.concurrent_prompter(\n",
    "    prompt_creator_from_searchItem=get_injection_strategy_devign,\n",
    "    index_retreiver=get_index,\n",
    "    vul_retriever=get_vul,\n",
    "    clean_retriever=get_clean_megavul,\n",
    "    vul_lines_retriever=get_vul_lines,\n",
    "    search_results=search_results,\n",
    "    llm=ch, target=6000, max_workers=8, is_all=False, resume_from_dataframe=None)  # for code Qwen set workers to 1 as there's only one model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tách file thành công!\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# # Đường dẫn file JSON gốc (cập nhật lại đường dẫn theo hệ thống của bạn)\n",
    "# input_file = r\"C:\\Users\\Administrator\\Downloads\\VulScribeR-main\\dataset\\megavul_simple.json\"\n",
    "\n",
    "# # Load file JSON và chuyển thành DataFrame\n",
    "# with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#     json_data = json.load(f)\n",
    "# mega_vul_df = pd.DataFrame(json_data)\n",
    "\n",
    "# # Tách dữ liệu dựa trên trường 'is_vul'\n",
    "# vul_df = mega_vul_df[mega_vul_df['is_vul'] == True]\n",
    "# clean_df = mega_vul_df[mega_vul_df['is_vul'] == False]\n",
    "\n",
    "# # Đường dẫn cho file output (cập nhật lại đường dẫn nếu cần)\n",
    "# vul_output_file = r\"C:\\Users\\Administrator\\Downloads\\VulScribeR-main\\code\\dataset\\megavul\\new\\megavul_vul.json\"\n",
    "# clean_output_file = r\"C:\\Users\\Administrator\\Downloads\\VulScribeR-main\\code\\dataset\\megavul\\new\\megavul_clean.json\"\n",
    "\n",
    "# # Ghi kết quả ra file JSON với định dạng 'records'\n",
    "# vul_df.to_json(vul_output_file, orient=\"records\", force_ascii=False, lines=True)\n",
    "# clean_df.to_json(clean_output_file, orient=\"records\", force_ascii=False, lines=True)\n",
    "\n",
    "# print(\"Tách file thành công!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results of the Generator\n",
    "df_result = pd.DataFrame({'vul': vuls, 'vul_lines': vul_lines, 'clean': cleans, 'generated': generated, 'idx':indices})\n",
    "df_result.to_json(f\"./generated/6k-devign-extension-randomfair.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused strategies\n",
    "\n",
    "# def get_injection_bigvul(search_item: SearchItem):\n",
    "#     clean_item = get_clean_bigvul(search_item)\n",
    "#     vul_code = get_vul(search_item)\n",
    "#     flaw_lines = get_vul_lines(search_item)\n",
    "#     return pu.get_full_prompt(vul_code, clean_item, flaw_lines)\n",
    "\n",
    "# def get_injection_ext(search_item: SearchItem):\n",
    "#     clean_item = get_clean_ext(search_item)\n",
    "#     vul_code = get_vul(search_item)\n",
    "#     flaw_lines = get_vul_lines(search_item)\n",
    "#     return pu.get_full_prompt(vul_code, clean_item, flaw_lines)\n",
    "# failed strategies (or the ones that didn't do any better)\n",
    "# def get_prompt4_devign(search_item: SearchItem):\n",
    "#     clean_item = get_clean_megavul(search_item)\n",
    "#     vul_code = get_vul(search_item)\n",
    "#     flaw_lines = get_vul_lines(search_item)\n",
    "#     return pu.get_full_prompt_without_examples_and_vardef_guide(vul_code, clean_item, flaw_lines)\n",
    "\n",
    "# def get_prompt3_devign(search_item: SearchItem):\n",
    "#     clean_item = get_clean_megavul(search_item)\n",
    "#     vul_code = get_vul(search_item)\n",
    "#     flaw_lines = get_vul_lines(search_item)\n",
    "#     fixed = get_fixed_vul(search_item)\n",
    "#     return pu.get_full_prompt_with_examples_and_vardef_guide(fixed, vul_code, clean_item, flaw_lines)\n",
    "\n",
    "# def get_prompt5_devign(search_item: SearchItem):\n",
    "#     clean_item = get_clean_megavul(search_item)\n",
    "#     vul_code = get_vul(search_item)\n",
    "#     return pu.get_ext_prompt(clean_item, vul_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vulscriber-cluster-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
